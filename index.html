<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematics & Statistics in ML and LLMs</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/theme/black.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/highlight/monokai.min.css">
    <style>
        .reveal h1, .reveal h2, .reveal h3 {
            text-transform: none;
        }
        .reveal .slides section {
            text-align: left;
        }
        .reveal .slides section.center {
            text-align: center;
        }
        .equation-box {
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }
        .highlight-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
        }
        .contact-info {
            font-size: 0.8em;
            color: #888;
            margin-top: 30px;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            
            <!-- Title Slide -->
            <section class="center" data-background-gradient="linear-gradient(135deg, #667eea 0%, #764ba2 100%)">
                <h1>Mathematics & Statistics in Machine Learning and LLMs</h1>
                <h3>Quarterly Technical Review</h3>
                <p style="margin-top: 50px;">Prepared by: Technical Consulting Team</p>
                <p class="contact-info">23f2005452@ds.study.iitm.ac.in</p>
                <aside class="notes">
                    Welcome everyone to this quarterly technical presentation. Today we'll explore the mathematical foundations that power modern ML and LLMs, which are increasingly important for our financial modeling and risk assessment systems.
                </aside>
            </section>

            <!-- Agenda Slide -->
            <section data-markdown>
                <textarea data-template>
                    ## Agenda
                    
                    - **Linear Algebra**: The Foundation
                    - **Calculus**: Optimization & Gradients
                    - **Probability & Statistics**: Understanding Uncertainty
                    - **Information Theory**: Measuring Information
                    - **Practical Applications**: Code Examples
                    - **LLM-Specific Mathematics**: Transformers & Attention
                </textarea>
                <aside class="notes">
                    We'll cover six key areas today. Each section builds on the previous one, showing how these mathematical concepts work together in modern AI systems.
                </aside>
            </section>

            <!-- Linear Algebra Section -->
            <section>
                <section class="center">
                    <h2>Linear Algebra: The Foundation</h2>
                    <p>Every neural network operation is a matrix manipulation</p>
                    <aside class="notes">
                        Linear algebra is the backbone of machine learning. Neural networks are essentially sequences of matrix multiplications and transformations.
                    </aside>
                </section>
                
                <section>
                    <h3>Core Concepts</h3>
                    <div class="fragment">
                        <h4>1. Vector Representations</h4>
                        <div class="equation-box">
                            <p>Input features as vectors:</p>
                            <p style="text-align: center; font-size: 1.2em;">
                                \(\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}\)
                            </p>
                        </div>
                    </div>
                    <div class="fragment">
                        <h4>2. Matrix Transformations</h4>
                        <div class="equation-box">
                            <p>Neural network layer:</p>
                            <p style="text-align: center; font-size: 1.2em;">
                                \(\mathbf{y} = W\mathbf{x} + \mathbf{b}\)
                            </p>
                        </div>
                    </div>
                    <aside class="notes">
                        Vectors represent data points, and matrices transform these vectors. Each layer in a neural network applies a linear transformation followed by a non-linear activation.
                    </aside>
                </section>

                <section>
                    <h3>Eigenvalues in PCA</h3>
                    <div class="highlight-box fragment">
                        <p>Principal Component Analysis reduces dimensionality by finding eigenvectors of the covariance matrix</p>
                    </div>
                    <div class="equation-box fragment">
                        <p style="text-align: center; font-size: 1.1em;">
                            \(\Sigma \mathbf{v} = \lambda \mathbf{v}\)
                        </p>
                        <p style="text-align: center; font-size: 0.9em;">
                            Where \(\Sigma\) is the covariance matrix, \(\mathbf{v}\) is an eigenvector, and \(\lambda\) is an eigenvalue
                        </p>
                    </div>
                    <aside class="notes">
                        PCA is crucial for feature reduction in high-dimensional financial data. We use this regularly for risk factor analysis.
                    </aside>
                </section>
            </section>

            <!-- Calculus Section -->
            <section>
                <section class="center">
                    <h2>Calculus: The Engine of Learning</h2>
                    <p>How neural networks learn from data</p>
                    <aside class="notes">
                        Calculus, specifically gradient descent, is how neural networks adjust their parameters to minimize error.
                    </aside>
                </section>
                
                <section>
                    <h3>Gradient Descent</h3>
                    <div class="fragment">
                        <p>The optimization algorithm that powers training:</p>
                        <div class="equation-box">
                            <p style="text-align: center; font-size: 1.2em;">
                                \(\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta_t)\)
                            </p>
                        </div>
                        <ul style="margin-top: 20px;">
                            <li class="fragment">\(\theta\): Model parameters (weights)</li>
                            <li class="fragment">\(\eta\): Learning rate</li>
                            <li class="fragment">\(\nabla_\theta J(\theta)\): Gradient of loss function</li>
                        </ul>
                    </div>
                    <aside class="notes">
                        This simple equation is the core of neural network training. We compute gradients using backpropagation and update weights iteratively.
                    </aside>
                </section>

                <section>
                    <h3>Chain Rule in Backpropagation</h3>
                    <div class="equation-box">
                        <p>For a composite function \(f(g(x))\):</p>
                        <p style="text-align: center; font-size: 1.2em; margin: 20px 0;">
                            \(\frac{\partial f}{\partial x} = \frac{\partial f}{\partial g} \cdot \frac{\partial g}{\partial x}\)
                        </p>
                    </div>
                    <div class="highlight-box fragment">
                        <p><strong>In Neural Networks:</strong> Gradients flow backward through layers, multiplying partial derivatives at each step</p>
                    </div>
                    <aside class="notes">
                        The chain rule allows us to compute gradients efficiently in deep networks. This is the mathematical foundation of backpropagation.
                    </aside>
                </section>
            </section>

            <!-- Probability & Statistics -->
            <section>
                <section class="center">
                    <h2>Probability & Statistics</h2>
                    <p>Quantifying Uncertainty</p>
                    <aside class="notes">
                        In financial AI, understanding and quantifying uncertainty is critical. Probability theory provides this framework.
                    </aside>
                </section>

                <section>
                    <h3>Key Statistical Concepts</h3>
                    <div class="fragment">
                        <h4>1. Maximum Likelihood Estimation (MLE)</h4>
                        <div class="equation-box">
                            <p style="text-align: center; font-size: 1.1em;">
                                \(\hat{\theta}_{MLE} = \arg\max_\theta \prod_{i=1}^{n} P(x_i|\theta)\)
                            </p>
                        </div>
                    </div>
                    <div class="fragment">
                        <h4>2. Bayes' Theorem</h4>
                        <div class="equation-box">
                            <p style="text-align: center; font-size: 1.1em;">
                                \(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\)
                            </p>
                        </div>
                    </div>
                    <aside class="notes">
                        MLE is fundamental to training models. Bayes' theorem underlies Bayesian neural networks and helps us update beliefs with new data.
                    </aside>
                </section>

                <section>
                    <h3>Normal Distribution in ML</h3>
                    <div class="equation-box">
                        <p style="text-align: center; font-size: 1.1em;">
                            \(f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)
                        </p>
                    </div>
                    <ul class="fragment">
                        <li>Weight initialization (Xavier/He initialization)</li>
                        <li>Gaussian processes</li>
                        <li>Noise modeling in data</li>
                        <li>Central Limit Theorem applications</li>
                    </ul>
                    <aside class="notes">
                        The normal distribution appears everywhere in ML. We use it for initializing weights, modeling uncertainty, and many statistical tests.
                    </aside>
                </section>
            </section>

            <!-- Code Example Section -->
            <section>
                <section class="center">
                    <h2>Practical Implementation</h2>
                    <p>From Theory to Code</p>
                    <aside class="notes">
                        Now let's see how these mathematical concepts translate into actual code implementations.
                    </aside>
                </section>

                <section>
                    <h3>Gradient Descent Implementation</h3>
                    <pre><code class="python" data-trim data-line-numbers="1-15|7-8|10-11|13-14">
import numpy as np

def gradient_descent(X, y, learning_rate=0.01, epochs=1000):
    m, n = X.shape
    theta = np.zeros(n)
    
    for epoch in range(epochs):
        # Forward pass: predictions
        predictions = X.dot(theta)
        
        # Compute gradient
        gradient = (1/m) * X.T.dot(predictions - y)
        
        # Update parameters
        theta = theta - learning_rate * gradient
        
        # Compute loss (MSE)
        loss = np.mean((predictions - y) ** 2)
        
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    return theta
                    </code></pre>
                    <aside class="notes">
                        This is a simple implementation of gradient descent for linear regression. Notice how we compute gradients and update parameters, just as in our mathematical formulation.
                    </aside>
                </section>

                <section>
                    <h3>Neural Network Layer (PyTorch-style)</h3>
                    <pre><code class="python" data-trim data-line-numbers="1-20|5-7|9-11|13-16">
import numpy as np

class LinearLayer:
    def __init__(self, input_dim, output_dim):
        # Initialize weights with Xavier initialization
        self.W = np.random.randn(input_dim, output_dim) * \
                 np.sqrt(2.0 / input_dim)
        self.b = np.zeros(output_dim)
        
    def forward(self, X):
        # Linear transformation: y = Wx + b
        return X.dot(self.W) + self.b
    
    def backward(self, grad_output):
        # Compute gradients using chain rule
        self.grad_W = self.X.T.dot(grad_output)
        self.grad_b = np.sum(grad_output, axis=0)
        grad_input = grad_output.dot(self.W.T)
        return grad_input
                    </code></pre>
                    <aside class="notes">
                        This demonstrates the forward and backward passes in a neural network layer. The backward pass uses the chain rule to compute gradients.
                    </aside>
                </section>

                <section>
                    <h3>Softmax & Cross-Entropy Loss</h3>
                    <pre><code class="python" data-trim data-line-numbers>
def softmax(z):
    """Convert logits to probabilities"""
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

def cross_entropy_loss(predictions, targets):
    """Compute cross-entropy loss"""
    m = targets.shape[0]
    log_likelihood = -np.log(predictions[range(m), targets])
    loss = np.sum(log_likelihood) / m
    return loss

# Example usage
logits = np.array([[2.0, 1.0, 0.1], [1.0, 3.0, 0.2]])
probs = softmax(logits)
# Output: probabilities sum to 1 for each sample
                    </code></pre>
                    <div class="equation-box fragment" style="margin-top: 20px;">
                        <p>Softmax formula: \(\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}\)</p>
                    </div>
                    <aside class="notes">
                        Softmax converts raw scores to probabilities. Cross-entropy measures how different predicted probabilities are from actual labels. This is the standard loss function for classification.
                    </aside>
                </section>
            </section>

            <!-- Information Theory -->
            <section>
                <section class="center">
                    <h2>Information Theory</h2>
                    <p>Measuring and Quantifying Information</p>
                    <aside class="notes">
                        Information theory, developed by Claude Shannon, provides the mathematical framework for understanding information and entropy in ML systems.
                    </aside>
                </section>

                <section>
                    <h3>Entropy & Cross-Entropy</h3>
                    <div class="fragment">
                        <h4>Entropy (Uncertainty)</h4>
                        <div class="equation-box">
                            <p style="text-align: center; font-size: 1.1em;">
                                \(H(p) = -\sum_{i=1}^{n} p(x_i) \log p(x_i)\)
                            </p>
                        </div>
                    </div>
                    <div class="fragment">
                        <h4>Cross-Entropy Loss</h4>
                        <div class="equation-box">
                            <p style="text-align: center; font-size: 1.1em;">
                                \(H(p,q) = -\sum_{i=1}^{n} p(x_i) \log q(x_i)\)
                            </p>
                        </div>
                    </div>
                    <p class="fragment">Used to measure difference between predicted (\(q\)) and true (\(p\)) distributions</p>
                    <aside class="notes">
                        Entropy measures uncertainty. Cross-entropy measures how well our model's predicted distribution matches the true distribution. Lower cross-entropy means better predictions.
                    </aside>
                </section>

                <section>
                    <h3>KL Divergence</h3>
                    <div class="equation-box">
                        <p style="text-align: center; font-size: 1.1em;">
                            \(D_{KL}(P||Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}\)
                        </p>
                    </div>
                    <div class="highlight-box fragment">
                        <p><strong>Applications in LLMs:</strong></p>
                        <ul>
                            <li>Variational Autoencoders (VAE)</li>
                            <li>Model distillation</li>
                            <li>Measuring distribution shift</li>
                            <li>Regularization in training</li>
                        </ul>
                    </div>
                    <aside class="notes">
                        KL divergence measures how one probability distribution differs from another. It's asymmetric and always non-negative. Critical for variational inference and model compression.
                    </aside>
                </section>
            </section>

            <!-- LLM-Specific Math -->
            <section>
                <section class="center">
                    <h2>Mathematics of LLMs</h2>
                    <p>Transformers & Attention Mechanisms</p>
                    <aside class="notes">
                        LLMs like GPT use transformer architecture, which relies on sophisticated mathematical concepts, particularly the attention mechanism.
                    </aside>
                </section>

                <section>
                    <h3>Self-Attention Mechanism</h3>
                    <div class="equation-box">
                        <p style="text-align: center; font-size: 1.1em;">
                            \(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)
                        </p>
                    </div>
                    <ul class="fragment">
                        <li><strong>Q</strong> (Query): What am I looking for?</li>
                        <li><strong>K</strong> (Key): What do I contain?</li>
                        <li><strong>V</strong> (Value): What information do I actually have?</li>
                        <li><strong>\(d_k\)</strong>: Scaling factor (dimension of keys)</li>
                    </ul>
                    <aside class="notes">
                        The attention mechanism allows the model to focus on relevant parts of the input. The scaling by square root of dimension prevents gradients from becoming too small.
                    </aside>
                </section>

                <section>
                    <h3>Positional Encoding</h3>
                    <div class="equation-box">
                        <p>Since transformers have no inherent sequence order:</p>
                        <p style="text-align: center; font-size: 0.95em; margin: 15px 0;">
                            \(PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)\)
                        </p>
                        <p style="text-align: center; font-size: 0.95em;">
                            \(PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)\)
                        </p>
                    </div>
                    <div class="highlight-box fragment">
                        <p>These sinusoidal functions encode position information, allowing the model to understand word order</p>
                    </div>
                    <aside class="notes">
                        Positional encoding adds information about token position in the sequence. Sinusoidal functions are chosen because they allow the model to learn relative positions easily.
                    </aside>
                </section>

                <section>
                    <h3>Layer Normalization</h3>
                    <div class="equation-box">
                        <p style="text-align: center; font-size: 1.1em;">
                            \(\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta\)
                        </p>
                    </div>
                    <pre class="fragment"><code class="python" data-trim>
def layer_norm(x, gamma, beta, eps=1e-5):
    """Layer normalization for stable training"""
    mean = np.mean(x, axis=-1, keepdims=True)
    var = np.var(x, axis=-1, keepdims=True)
    x_norm = (x - mean) / np.sqrt(var + eps)
    return gamma * x_norm + beta
                    </code></pre>
                    <p class="fragment">Critical for training stability in deep transformers</p>
                    <aside class="notes">
                        Layer normalization stabilizes training by normalizing activations. It's applied within each layer and is crucial for training very deep networks like GPT-4.
                    </aside>
                </section>
            </section>

            <!-- Financial Applications -->
            <section>
                <section class="center">
                    <h2>Financial Applications</h2>
                    <p>Mathematics in Risk & Portfolio Management</p>
                    <aside class="notes">
                        Let's connect this to our financial domain. These mathematical concepts directly apply to our risk models and trading systems.
                    </aside>
                </section>

                <section>
                    <h3>Portfolio Optimization</h3>
                    <div class="equation-box fragment">
                        <p><strong>Modern Portfolio Theory (Markowitz):</strong></p>
                        <p style="text-align: center; font-size: 1.1em;">
                            \(\min_w \frac{1}{2}w^T\Sigma w - \lambda \mu^T w\)
                        </p>
                        <p style="text-align: center; font-size: 0.9em;">
                            subject to: \(\sum_i w_i = 1\), \(w_i \geq 0\)
                        </p>
                    </div>
                    <ul class="fragment">
                        <li>\(w\): Portfolio weights</li>
                        <li>\(\Sigma\): Covariance matrix of returns</li>
                        <li>\(\mu\): Expected returns vector</li>
                        <li>\(\lambda\): Risk aversion parameter</li>
                    </ul>
                    <aside class="notes">
                        This optimization problem is solved using gradient-based methods, just like training neural networks. We can use ML to predict the covariance matrix and expected returns.
                    </aside>
                </section>

                <section>
                    <h3>Value at Risk (VaR) with ML</h3>
                    <div class="equation-box">
                        <p style="text-align: center; font-size: 1.1em;">
                            \(VaR_\alpha = -\inf\{x \in \mathbb{R}: P(L \leq x) \geq \alpha\}\)
                        </p>
                    </div>
                    <div class="highlight-box fragment">
                        <p><strong>ML Enhancement:</strong> Use neural networks to model complex return distributions, capturing fat tails and non-linear dependencies that traditional methods miss</p>
                    </div>
                    <pre class="fragment"><code class="python" data-trim>
# ML-based VaR estimation
model = train_deep_quantile_regression(returns_data)
var_95 = model.predict_quantile(portfolio, alpha=0.05)
                    </code></pre>
                    <aside class="notes">
                        Traditional VaR assumes normal distribution. ML models can capture complex patterns in returns, giving more accurate risk estimates.
                    </aside>
                </section>
            </section>

            <!-- Key Takeaways -->
            <section>
                <section class="center" data-background-gradient="linear-gradient(135deg, #667eea 0%, #764ba2 100%)">
                    <h2>Key Takeaways</h2>
                    <aside class="notes">
                        Let's summarize the most important points from today's presentation.
                    </aside>
                </section>

                <section>
                    <h3>Mathematical Foundations</h3>
                    <div class="fragment highlight-box">
                        <p><strong>1. Linear Algebra:</strong> Data representation and transformations</p>
                    </div>
                    <div class="fragment highlight-box">
                        <p><strong>2. Calculus:</strong> Optimization through gradient descent</p>
                    </div>
                    <div class="fragment highlight-box">
                        <p><strong>3. Probability:</strong> Uncertainty quantification and inference</p>
                    </div>
                    <div class="fragment highlight-box">
                        <p><strong>4. Information Theory:</strong> Measuring and comparing distributions</p>
                    </div>
                    <aside class="notes">
                        These four pillars support all modern ML and LLM systems. Mastering them is essential for developing and deploying AI in financial applications.
                    </aside>
                </section>

                <section>
                    <h3>LLM-Specific Insights</h3>
                    <ul>
                        <li class="fragment">Self-attention enables context understanding</li>
                        <li class="fragment">Positional encoding maintains sequence information</li>
                        <li class="fragment">Layer normalization ensures training stability</li>
                        <li class="fragment">Transformer architecture scales to billions of parameters</li>
                    </ul>
                    <div class="fragment" style="margin-top: 30px; padding: 20px; background: rgba(102, 126, 234, 0.2); border-radius: 10px;">
                        <p><strong>Impact on Finance:</strong> These techniques power document analysis, sentiment analysis, risk report generation, and automated compliance checking</p>
                    </div>
                    <aside class="notes">
                        LLMs are revolutionizing how we process financial documents, analyze market sentiment, and automate regulatory compliance. Understanding their mathematical foundations helps us deploy them effectively.
                    </aside>
                </section>
            </section>

            <!-- Conclusion -->
            <section class="center" data-background-gradient="linear-gradient(135deg, #764ba2 0%, #667eea 100%)">
                <h2>Thank You</h2>
                <h3 style="margin-top: 50px;">Questions?</h3>
                <p style="margin-top: 80px; font-size: 0.8em;">
                    Contact: 23f2005452@ds.study.iitm.ac.in
                </p>
                <p style="font-size: 0.7em; margin-top: 20px; color: #ccc;">
                    Technical Consulting Team | Q4 2024
                </p>
                <aside class="notes">
                    Thank you for your attention. I'm happy to answer any questions about how we can apply these mathematical concepts to our financial models and risk systems.
                </aside>
            </section>

        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/notes/notes.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/markdown/markdown.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/highlight/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/math/math.min.js"></script>

    <script>
        Reveal.initialize({
            hash: true,
            center: false,
            transition: 'slide',
            transitionSpeed: 'default',
            backgroundTransition: 'fade',
            slideNumber: true,
            controls: true,
            progress: true,
            history: true,
            
            math: {
                mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js',
                config: 'TeX-AMS_HTML-full',
                TeX: {
                    Macros: {
                        R: '\\mathbb{R}',
                        set: ['\\left\\{#1 \\; ; \\; #2\\right\\}', 2]
                    }
                }
            },
            
            plugins: [
                RevealMarkdown, 
                RevealHighlight, 
                RevealNotes, 
                RevealMath
            ]
        });

        // Force MathJax to re-render after slide changes
        Reveal.on('slidechanged', function() {
            if (window.MathJax) {
                MathJax.typesetPromise();
            }
        });

        // Initial render after everything loads
        Reveal.on('ready', function() {
            setTimeout(function() {
                if (window.MathJax) {
                    MathJax.typesetPromise();
                }
            }, 500);
        });
    </script>
</body>
</html>
